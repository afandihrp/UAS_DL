{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070cbac8",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT pada MNLI (end-to-end)\n",
    "\n",
    "Notebook ini menunjukkan pipeline terawasi untuk fine-tuning model keluarga BERT pada dataset MNLI (GLUE). Setiap langkah dilengkapi penjelasan singkat sebelum cell kode terkait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2140bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /home/apalah/Documents/uasdl/task1/mnli/virtualenvdl/bin/pip: cannot execute: required file not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (jalankan sekali jika perlu)\n",
    "!pip install -q transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be7b89",
   "metadata": {},
   "source": [
    "## Imports dan konfigurasi awal\n",
    "Cell ini memuat library yang digunakan: `datasets` untuk MNLI, `transformers` untuk tokenizer/model, dan utilitas evaluasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39428c",
   "metadata": {},
   "source": [
    "## Tahap 1: Deteksi GPU\n",
    "Cell ini akan memeriksa ketersediaan GPU (CUDA) dan menampilkan perangkat yang akan digunakan untuk training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 1 GPU yang tersedia untuk training.\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Periksa ketersediaan CUDA (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    # Dapatkan jumlah GPU yang tersedia\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Ditemukan {gpu_count} GPU yang tersedia untuk training.\")\n",
    "    # Tampilkan detail untuk setiap GPU\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"Tidak ada GPU yang ditemukan. Training akan berjalan di CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9eac5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Checkpoint BERT (ubah jika ingin varian lain dari keluarga BERT)\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "output_dir = \"./mnli-bert-finetuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93623520",
   "metadata": {},
   "source": [
    "## Muat dataset MNLI dari GLUE\n",
    "Kita akan memuat split `train` dan `validation` (matched/mismatched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a899d86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a9fd7",
   "metadata": {},
   "source": [
    "## Preprocessing & Tokenisasi\n",
    "Tokenisasi pasangan teks (`premise`, `hypothesis`) dan penyiapan field `labels` yang diperlukan Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b04546aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9815/9815 [00:00<00:00, 24198.40 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Fungsi tokenisasi untuk pasangan premise/hypothesis\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['premise'], example['hypothesis'], truncation=True)\n",
    "\n",
    "# Terapkan tokenisasi secara batch untuk efisiensi\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Trainer mengharapkan kolom 'labels' — MNLI pada GLUE sudah memiliki 'label'\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "# Hapus kolom yang tidak dipakai dan set format ke PyTorch\n",
    "cols_to_remove = [c for c in tokenized_datasets['train'].column_names if c not in ['input_ids','attention_mask','token_type_ids','labels']]\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(cols_to_remove)\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b765d9",
   "metadata": {},
   "source": [
    "## Siapkan model dan TrainingArguments\n",
    "Buat model klasifikasi dengan `num_labels=3` (entailment/neutral/contradiction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb3d1ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_44463/459885102.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1m = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {**acc, **{'f1_macro': f1m['f1']}}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation_matched'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dff93",
   "metadata": {},
   "source": [
    "## Training\n",
    "Jalankan `trainer.train()` untuk fine-tuning. Sesuaikan `num_train_epochs` dan batch size sesuai resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c0abb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49088' max='49088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49088/49088 54:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.456390</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.831706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49088, training_loss=0.5280754309745003, metrics={'train_runtime': 3249.7041, 'train_samples_per_second': 120.842, 'train_steps_per_second': 15.105, 'total_flos': 1.417814915563998e+16, 'train_loss': 0.5280754309745003, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Simpan tokenizer juga\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b343a59",
   "metadata": {},
   "source": [
    "## Evaluasi akhir dan contoh inferensi\n",
    "Evaluasi pada split validasi (matched) dan berikan contoh prediksi dari kalimat pasangan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "383834e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.4563902020454407, 'eval_accuracy': 0.8322975038206827, 'eval_f1_macro': 0.8317062443100176, 'eval_runtime': 14.0998, 'eval_samples_per_second': 696.107, 'eval_steps_per_second': 43.547, 'epoch': 1.0}\n",
      "Premise: Two men in polo shirts standing in a bar.\n",
      "Hypothesis: They are in a pub.\n",
      "Scores: [{'label': 'LABEL_0', 'score': 0.7811073660850525}, {'label': 'LABEL_1', 'score': 0.05497472733259201}, {'label': 'LABEL_2', 'score': 0.16391794383525848}]\n",
      "Predicted Label: entailment (Score: 0.7811)\n",
      "---\n",
      "Premise: A man inspects the uniform of a figure in some East Asian country.\n",
      "Hypothesis: A man is sleeping.\n",
      "Scores: [{'label': 'LABEL_0', 'score': 0.004381849430501461}, {'label': 'LABEL_1', 'score': 0.026000216603279114}, {'label': 'LABEL_2', 'score': 0.9696179628372192}]\n",
      "Predicted Label: contradiction (Score: 0.9696)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apalah/Documents/uasdl/task1/mnli/virtualenvdl/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# Contoh prediksi cepat menggunakan pipeline dari tokenizer + model yang ter-save\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set device ke GPU jika tersedia, jika tidak, gunakan CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline('text-classification', model=output_dir, tokenizer=output_dir, return_all_scores=True, device=device)\n",
    "\n",
    "# Mapping manual dari ID ke label untuk MNLI\n",
    "id2label = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "examples = [\n",
    "    (\"Two men in polo shirts standing in a bar.\", \"They are in a pub.\"),\n",
    "    (\"A man inspects the uniform of a figure in some East Asian country.\", \"A man is sleeping.\")\n",
    "]\n",
    "\n",
    "for a,b in examples:\n",
    "    # Cara yang lebih tepat untuk pipeline: lewatkan sebagai pasangan kalimat\n",
    "    out = classifier({\"text\": a, \"text_pair\": b})\n",
    "    print('Premise:', a)\n",
    "    print('Hypothesis:', b)\n",
    "    \n",
    "    # Temukan prediksi dengan skor tertinggi\n",
    "    best_prediction = max(out, key=lambda x: x['score'])\n",
    "    predicted_label_id = int(best_prediction['label'].split('_')[-1])\n",
    "    predicted_label_name = id2label[predicted_label_id]\n",
    "\n",
    "    print('Scores:', out)\n",
    "    print(f\"Predicted Label: {predicted_label_name} (Score: {best_prediction['score']:.4f})\")\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenvdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
