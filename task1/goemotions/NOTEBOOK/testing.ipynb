{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c2110c",
   "metadata": {},
   "source": [
    "# Fine-tune BERT untuk GoEmotions (classifier)\n",
    "Notebook ini menunjukkan pipeline end-to-end untuk fine-tuning model keluarga BERT pada dataset GoEmotions (multi-label).\n",
    "Setiap langkah disertai penjelasan singkat pada sel Markdown di atas sel kode terkait."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76770d47",
   "metadata": {},
   "source": [
    "## 1) Instalasi dependensi\n",
    "Jika environment belum memiliki paket yang dibutuhkan jalankan sel berikut. Jika sudah ada, lewati saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c8c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /home/apalah/Documents/uasdl/task1/goemotions/virtualenvdl/bin/pip: cannot execute: required file not found\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (jalankan hanya jika perlu)\n",
    "!pip install -q transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a8af9",
   "metadata": {},
   "source": [
    "## 2) Import & konfigurasi awal\n",
    "Impor library utama, set seed untuk reproducibility, dan tentukan device (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583be129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apalah/Documents/uasdl/task1/goemotions/virtualenvdl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Reproducibility & device\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab9bb3",
   "metadata": {},
   "source": [
    "## 3) Memuat dataset GoEmotions\n",
    "Gunakan `datasets` untuk memuat `google-research-datasets/go_emotions`. Kita akan melihat nama label dan jumlah label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e611cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels = 28\n",
      "Example labels: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "# Load GoEmotions dataset from Hugging Face\n",
    "dataset = load_dataset('google-research-datasets/go_emotions')\n",
    "label_names = dataset['train'].features['labels'].feature.names\n",
    "num_labels = len(label_names)\n",
    "print('Num labels =', num_labels)\n",
    "print('Example labels:', label_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3a868",
   "metadata": {},
   "source": [
    "## 4) Preprocessing & Tokenization\n",
    "Tokenize teks dan konversi daftar indeks label menjadi vektor multi-hot untuk klasifikasi multi-label. Batasi `max_length` agar input konsisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f31af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5426/5426 [00:00<00:00, 13490.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: tokenizer + convert label lists -> multi-hot vectors\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(batch):\n",
    "    enc = tokenizer(batch['text'], truncation=True, max_length=128)\n",
    "    multi = []\n",
    "    for lab in batch['labels']:\n",
    "        v = [0.0]*num_labels\n",
    "        for i in lab:\n",
    "            v[i] = 1.0\n",
    "        multi.append(v)\n",
    "    # Ensure labels are float (BCEWithLogits expects float targets)\n",
    "    enc['labels'] = multi\n",
    "    return enc\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset['train'].column_names)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9e117",
   "metadata": {},
   "source": [
    "## 5) Menyiapkan model untuk multi-label\n",
    "Buat konfigurasi `AutoConfig` dengan `problem_type='multi_label_classification'` dan muat `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43684e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model setup (multi-label)\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, problem_type='multi_label_classification')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config).to(device)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Ensure labels are float in batches: wrap collator to cast labels to float32 (BCEWithLogits needs float targets)\n",
    "def float_data_collator(features):\n",
    "    batch = data_collator(features)\n",
    "    if 'labels' in batch:\n",
    "        batch['labels'] = batch['labels'].to(torch.float)\n",
    "    return batch\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f727a",
   "metadata": {},
   "source": [
    "## 6) Metrik Evaluasi (F1 multi-label)\n",
    "Terapkan sigmoid ke logits, gunakan threshold 0.5 untuk mendapatkan prediksi biner, lalu hitung F1 (micro & macro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b381b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for multi-label: use sigmoid + threshold then compute F1 (micro & macro)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    probs = sigmoid(logits)\n",
    "    y_pred = (probs >= 0.5).astype(int)\n",
    "    # label_ids may be floats (multi-hot); convert to binary ints for metrics\n",
    "    y_true = (pred.label_ids >= 0.5).astype(int)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return {'f1_micro': f1_micro, 'f1_macro': f1_macro}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc8228",
   "metadata": {},
   "source": [
    "## 7) Konfigurasi Training dan Menjalankan Trainer\n",
    "Tentukan `TrainingArguments` (batch size, epochs, learning rate, strategi evaluasi). Sesuaikan untuk percobaan cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf0fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33560/1605087579.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 14:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.093321</td>\n",
       "      <td>0.509136</td>\n",
       "      <td>0.248086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.085575</td>\n",
       "      <td>0.556098</td>\n",
       "      <td>0.367664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.085382</td>\n",
       "      <td>0.573374</td>\n",
       "      <td>0.394483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08538229018449783,\n",
       " 'eval_f1_micro': 0.5733743819386137,\n",
       " 'eval_f1_macro': 0.3944834265551541,\n",
       " 'eval_runtime': 8.9752,\n",
       " 'eval_samples_per_second': 604.556,\n",
       " 'eval_steps_per_second': 18.941,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./goemotions-bert',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=float_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Note: training can be long. Reduce epochs or use smaller batch for quick tests.\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57b4e4",
   "metadata": {},
   "source": [
    "## 8) Inference contoh\n",
    "Setelah model terlatih, lakukan prediksi pada teks baru: ambil logits, terapkan sigmoid, dan threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cac7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['joy']\n",
      "Top probabilities:\n",
      "joy 0.635\n",
      "excitement 0.473\n",
      "gratitude 0.052\n",
      "neutral 0.039\n",
      "admiration 0.035\n"
     ]
    }
   ],
   "source": [
    "# Inference example\n",
    "text = 'I feel joyful and excited today!'\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits.cpu().numpy()[0]\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "preds = [label_names[i] for i,p in enumerate(probs) if p>0.5]\n",
    "print('Predicted labels:', preds)\n",
    "print('Top probabilities:')\n",
    "for i in np.argsort(probs)[-5:][::-1]:\n",
    "    print(label_names[i], f'{probs[i]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
