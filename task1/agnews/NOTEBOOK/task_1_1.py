# -*- coding: utf-8 -*-
"""task_1_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PGFxU4OJ3Lz7j6_NoENESCSv64sz3Wnr

# Jupyter Notebook: Fine-Tuning BERT for AG News Text Classification
## 1. Setup Environment
Install the necessary libraries as demonstrated in your learning materials.
"""

!pip install transformers datasets accelerate evaluate scikit-learn -q

import torch
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    pipeline
)
import evaluate
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Check GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

"""## 2. Load and Explore Dataset
The AG News dataset classifies news into 4 categories: World (0), Sports (1), Business (2), and Sci/Tech (3).
"""

dataset = load_dataset("ag_news")
label_names = ["World", "Sports", "Business", "Sci/Tech"]

# Display sample data (as provided in your prompt)
train_df = pd.DataFrame(dataset['train'])
test_df = pd.DataFrame(dataset['test'])
print("First 5 rows of Training Data:")
print(train_df.head())

"""## 3. Comparison: Traditional Machine Learning (Baseline)
The assignment requires a comparison between traditional and deep learning models.
"""

print("--- Training Traditional ML Baseline (Logistic Regression) ---")
vectorizer = TfidfVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(train_df['text'])
X_test = vectorizer.transform(test_df['text'])

model_lr = LogisticRegression(max_iter=1000)
model_lr.fit(X_train, train_df['label'])

lr_preds = model_lr.predict(X_test)
print(f"Logistic Regression Accuracy: {accuracy_score(test_df['label'], lr_preds):.4f}")

"""## 4. BERT Tokenization
Following the bert-base-uncased approach in your material.
"""

from sklearn.model_selection import train_test_split
from datasets import Dataset # Import Dataset constructor for creating datasets from pandas

MODEL_NAME = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

# Tokenize and format the full datasets
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["text"])

# Convert to pandas for stratified sampling
train_df_full = tokenized_datasets["train"].to_pandas()
test_df_full = tokenized_datasets["test"].to_pandas()

# Stratified sampling for training set
n_samples_train = 2000
_, small_train_df = train_test_split(
    train_df_full,
    test_size=n_samples_train, # Use test_size to get the desired number of samples for the smaller split
    stratify=train_df_full['label'],
    random_state=42
)
small_train_dataset = Dataset.from_pandas(small_train_df, preserve_index=False)


# Stratified sampling for evaluation set
n_samples_eval = 200
_, small_eval_df = train_test_split(
    test_df_full,
    test_size=n_samples_eval,
    stratify=test_df_full['label'],
    random_state=42
)
small_eval_dataset = Dataset.from_pandas(small_eval_df, preserve_index=False)

# Set format to torch for both small datasets
small_train_dataset.set_format("torch")
small_eval_dataset.set_format("torch")

"""## 5. Model Configuration and Training
Setting up the classification head for 4 labels as done in the multi-class sentiment notebook.
"""

model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)
model.to(device)

# Metrics
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./finetuning-bert-text-classification",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    fp16=torch.cuda.is_available()
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

# Start Fine-Tuning
trainer.train()

"""## 6. Final Evaluation & Saving
Compare the final BERT accuracy with the Logistic Regression baseline.
"""

# Final evaluation on test set
results = trainer.evaluate()
print(f"BERT Final Accuracy: {results['eval_accuracy']:.4f}")

# Save the model
model.save_pretrained("./ag-news-bert-model")
tokenizer.save_pretrained("./ag-news-bert-model")

"""## 7. Inference
Testing the model with custom text.
"""

classifier = pipeline("text-classification", model="./ag-news-bert-model", device=0 if torch.cuda.is_available() else -1)

# Mapping ID to Label Name
id2label = {0: "World", 1: "Sports", 2: "Business", 3: "Sci/Tech"}

test_text = "The new spacecraft successfully landed on Mars today to search for life."
prediction = classifier(test_text)
label_id = int(prediction[0]['label'].split('_')[-1])
print(f"Text: {test_text}")
print(f"Predicted Category: {id2label[label_id]}")