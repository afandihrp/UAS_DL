{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jupyter Notebook: Fine-Tuning BERT for AG News Text Classification\n",
        "## 1. Setup Environment\n",
        "Install the necessary libraries as demonstrated in your learning materials."
      ],
      "metadata": {
        "id": "z5o-LwDp4yxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGoFhZ02yG4E",
        "outputId": "afe606eb-2239-41fd-bb63-1ac1198b5012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate evaluate scikit-learn -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and Explore Dataset\n",
        "The AG News dataset classifies news into 4 categories: World (0), Sports (1), Business (2), and Sci/Tech (3)."
      ],
      "metadata": {
        "id": "RkKZpPxm44mr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29e7cd6",
        "outputId": "ecb75db0-1546-476c-88ed-51aec5b026d4"
      },
      "source": [
        "dataset = load_dataset(\"ag_news\")\n",
        "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "# Display sample data (as provided in your prompt)\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "print(\"First 5 rows of Training Data:\")\n",
        "print(train_df.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of Training Data:\n",
            "                                                text  label\n",
            "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
            "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
            "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
            "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
            "4  Oil prices soar to all-time record, posing new...      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Comparison: Traditional Machine Learning (Baseline)\n",
        "The assignment requires a comparison between traditional and deep learning models."
      ],
      "metadata": {
        "id": "L7c2o2eV4-Uy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ab3a8d",
        "outputId": "280bd46a-1f6a-4391-bc25-225436303300"
      },
      "source": [
        "print(\"--- Training Traditional ML Baseline (Logistic Regression) ---\")\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df['text'])\n",
        "X_test = vectorizer.transform(test_df['text'])\n",
        "\n",
        "model_lr = LogisticRegression(max_iter=1000)\n",
        "model_lr.fit(X_train, train_df['label'])\n",
        "\n",
        "lr_preds = model_lr.predict(X_test)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(test_df['label'], lr_preds):.4f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Traditional ML Baseline (Logistic Regression) ---\n",
            "Logistic Regression Accuracy: 0.9057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. BERT Tokenization\n",
        "Following the bert-base-uncased approach in your material."
      ],
      "metadata": {
        "id": "fh5NyIeT5GOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset # Import Dataset constructor for creating datasets from pandas\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenize and format the full datasets\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "\n",
        "# Convert to pandas for stratified sampling\n",
        "train_df_full = tokenized_datasets[\"train\"].to_pandas()\n",
        "test_df_full = tokenized_datasets[\"test\"].to_pandas()\n",
        "\n",
        "# Stratified sampling for training set\n",
        "n_samples_train = 2000\n",
        "_, small_train_df = train_test_split(\n",
        "    train_df_full,\n",
        "    test_size=n_samples_train, # Use test_size to get the desired number of samples for the smaller split\n",
        "    stratify=train_df_full['label'],\n",
        "    random_state=42\n",
        ")\n",
        "small_train_dataset = Dataset.from_pandas(small_train_df, preserve_index=False)\n",
        "\n",
        "\n",
        "# Stratified sampling for evaluation set\n",
        "n_samples_eval = 200\n",
        "_, small_eval_df = train_test_split(\n",
        "    test_df_full,\n",
        "    test_size=n_samples_eval,\n",
        "    stratify=test_df_full['label'],\n",
        "    random_state=42\n",
        ")\n",
        "small_eval_dataset = Dataset.from_pandas(small_eval_df, preserve_index=False)\n",
        "\n",
        "# Set format to torch for both small datasets\n",
        "small_train_dataset.set_format(\"torch\")\n",
        "small_eval_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fb1734ac090146ccb7b03687a2cd8a30",
            "c0a328d5024d4dafbc20d4ca086406a9",
            "7cec9504828d4d54b76237b77f48c2c4",
            "efd0da98bcc0485c9e5ec5d237c0e212",
            "1fab71e5db734b4586e993ab1cad0b56",
            "c7e53102f7ca4ab1b16ed9779c9e278b",
            "8042e3712f79414b803e392a68c6a192",
            "71a1f46b0aba4d5598cecf04be5f789b",
            "158371e580134ee98c14a26eb6d6de56",
            "b037c98f6cd84220b8f8b9d5b7f8f16b",
            "92637d647dde4814abf6410754976434",
            "9a4e614e24c9415593b76b08190d4255",
            "0be7baf0c48d41dca5c0131b88599ce9",
            "5a50a8a1a5cb47b9ab69f51c70493cd6",
            "edd82b99788d475abb8db90347b5d137",
            "e62e6eb99f804287bc17d90767af29a8",
            "8eada8b2fb5f4018a3391140e810d11f",
            "526585b7f5354149a2e108f7e513461a",
            "42e677cb1c7740ccaebfcadd07fa5127",
            "0c637731bfa54f668f8b0f20b7b4bac2",
            "6f5565ad4876479abb48adf92333fad2",
            "2627c684cbb1488e80a4b21d74629320"
          ]
        },
        "id": "QBfBf6cP4p7i",
        "outputId": "e86a145a-d70d-4648-ee43-2b8513909781"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb1734ac090146ccb7b03687a2cd8a30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a4e614e24c9415593b76b08190d4255"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Configuration and Training\n",
        "Setting up the classification head for 4 labels as done in the multi-class sentiment notebook."
      ],
      "metadata": {
        "id": "WIJWWKBO5Cpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
        "model.to(device)\n",
        "\n",
        "# Metrics\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuning-bert-text-classification\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start Fine-Tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "xgJCM0x84r9y",
        "outputId": "31a3fd10-bb02-40a1-836a-7bbd64b97188"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 1:41:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.305007</td>\n",
              "      <td>0.920000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.243130</td>\n",
              "      <td>0.945000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.253889</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=0.3549471435546875, metrics={'train_runtime': 6132.6886, 'train_samples_per_second': 0.978, 'train_steps_per_second': 0.061, 'total_flos': 394673670144000.0, 'train_loss': 0.3549471435546875, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Final Evaluation & Saving\n",
        "Compare the final BERT accuracy with the Logistic Regression baseline."
      ],
      "metadata": {
        "id": "q_tj7iw85MYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on test set\n",
        "results = trainer.evaluate()\n",
        "print(f\"BERT Final Accuracy: {results['eval_accuracy']:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./ag-news-bert-model\")\n",
        "tokenizer.save_pretrained(\"./ag-news-bert-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "ZvhzmabN4tky",
        "outputId": "d4969583-9000-4cf2-903a-9786bd2f68b4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:52]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Final Accuracy: 0.9450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./ag-news-bert-model/tokenizer_config.json',\n",
              " './ag-news-bert-model/special_tokens_map.json',\n",
              " './ag-news-bert-model/vocab.txt',\n",
              " './ag-news-bert-model/added_tokens.json',\n",
              " './ag-news-bert-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Inference\n",
        "Testing the model with custom text."
      ],
      "metadata": {
        "id": "ER6yMWxM5Rcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"text-classification\", model=\"./ag-news-bert-model\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Mapping ID to Label Name\n",
        "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
        "\n",
        "test_text = \"The new spacecraft successfully landed on Mars today to search for life.\"\n",
        "prediction = classifier(test_text)\n",
        "label_id = int(prediction[0]['label'].split('_')[-1])\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Predicted Category: {id2label[label_id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN6dhi_d4vcH",
        "outputId": "7bf723e2-1bc8-4248-d464-3c3d6108f3d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The new spacecraft successfully landed on Mars today to search for life.\n",
            "Predicted Category: Sci/Tech\n"
          ]
        }
      ]
    }
  ]
}
